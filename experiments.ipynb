{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75025606",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/karencfisher/landmark-classifier.git\n",
    "%cd landmark-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8a81cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import sqlite3\n",
    "\n",
    "from src.data import get_data_loaders\n",
    "from src.train import optimize\n",
    "from src.optimization import get_optimizer, get_loss\n",
    "from src.train import one_epoch_test\n",
    "from src.helpers import setup_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aff252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running locally, this will download dataset (make sure you have at\n",
    "# least 2 Gb of space on your hard drive)\n",
    "setup_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c002fbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/model1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/model1.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, num_classes: int = 1000, dropout: float = 0.7) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, 1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2, 2) \n",
    "                \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(197136, 1024)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # YOUR CODE HERE: process the input tensor through the\n",
    "        # feature extractor, the pooling and the final linear\n",
    "        # layers (if appropriate for the architecture chosen)\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "525f6b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (relu1): ReLU()\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=197136, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fc3): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run models/model1.py\n",
    "model = MyModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f786f27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32        # size of the minibatch for stochastic gradient descent (or Adam)\n",
    "valid_size = 0.2       # fraction of the training data to reserve for validation\n",
    "num_epochs = 3        # number of epochs for training\n",
    "num_classes = 50       # number of classes. Do not change this\n",
    "dropout = 0.4          # dropout for our model\n",
    "learning_rate = 0.001  # Learning rate for SGD (or Adam)\n",
    "opt = 'adam'            # optimizer. 'sgd' or 'adam'\n",
    "weight_decay = 0.0     # regularization. Increase this to combat overfitting\n",
    "scheduler = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6afc4e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.optim.lr_scheduler.ExponentialLR'>\n"
     ]
    }
   ],
   "source": [
    "scheduler = torch.optim.lr_scheduler.ExponentialLR\n",
    "print(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556cfd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing cached mean and std\n",
      "Dataset mean: tensor([0.4638, 0.4725, 0.4687]), std: tensor([0.2697, 0.2706, 0.3017])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                         | 0/125 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "data_loaders = get_data_loaders(batch_size, valid_size)\n",
    "optimizer = get_optimizer(model, opt, learning_rate, weight_decay=weight_decay)\n",
    "loss = get_loss()\n",
    "\n",
    "train_losses, valid_losses = optimize(\n",
    "    data_loaders,\n",
    "    model,\n",
    "    optimizer,\n",
    "    loss,\n",
    "    num_epochs,\n",
    "    save_path=\"checkpoints/best_val_loss.pt\",\n",
    "    scheduler=scheduler\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aec11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = range(1, num_epochs+1)\n",
    "plt.plot(x, train_losses, label='train loss')\n",
    "plt.plot(x, valid_losses, label='valid losses')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e138bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_loss, accuracy = one_epoch_test(data_loaders['test'], model, loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5b15d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log experiment\n",
    "file_name = input(\"File name: \")\n",
    "\n",
    "conn = sqlite3.connect('experiments.db')\n",
    "cursor = conn.cursor\n",
    "\n",
    "sql = '''\n",
    "INSERT INTO experiments (batch_size, num_epochs, dropout, learning_rate, optimizer, \n",
    "weight_decay, scheduler, Accuracy, train_losses, valid_losses, model_file)\n",
    "VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "'''\n",
    "\n",
    "values = (batch_size, num_epochs, dropout, learning_rate, opt, weight_decay, str(scheduler), \n",
    "          accuracy, train_losses, valid_losses, file_name)\n",
    "\n",
    "cursor.execute(sql, values)\n",
    "conn.commit()\n",
    "\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
